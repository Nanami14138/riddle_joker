# 吴恩达机器学习笔记
## 初步学习神经网络：单个神经网络的logistic 回归
* 基本例子，怎么输入输出:
  
    输入一组图片，判断图片上的动物是否是猫（二元分类）

    输入x(nx,m)为矩阵形式的数据集合，即m个样本，每个样本中有nx个指标

    $\hat{y}$=P(y=1|x) 

    0 $\le$ $\hat{y}$ $\le$ 1
* 符号以及公式
  
    $w$   -  (1,nx) 实矩阵

    b-(1,1)实数

    $\hat{y}$=$\sigma$(w$^T$+b)

    $\sigma$激活函数：
    $$\sigma(z)=\frac{1}{1+e^-z}$$

    回归损失函数$L$($\hat{y}$,$y$)

    $$L(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^2$$

    $$L(\hat{y},y)=-(ylog \hat{y}+(1-y)log(1- \hat{y}))$$

    成本函数
    $$J(w,b)=\frac{1}{m}\sum L(\hat{y}(i),y(i))\quad(0\le i \le m)$$

    y=1时 
    $$L(\hat{y},y)=-ylog \hat{y}$$

    y=0时 

    $$L(\hat{y},y)=-(1-y)log(1- \hat{y})$$

* 多个样本写代码使用for循环存在的问题以及如何对其进行梯度下降
  
    z=w$^T$x+b$\quad$w$\in$R$^n$$\quad$x$\in$R$^n$

    使用for循环即未经向量化处理的伪代码
    z=0
    for i in range(n):
    z+=w[i]*x[i]
    z+=b

* 向量化logistic回归



* 向量化logistic回归的梯度输出
```

```
## 初步搭建神经网络
### 激活函数的选择以及梯度下降法
* 激活函数的分类以及应用
  
    $\sigma$(z)  （0，1）          一般用作二元分类的输出层，不用在隐藏层

    tanh(z)     （-1，1）        可以用作隐藏层

    ReLU(z)=max(z,0)      （0，+$\infty$） z趋近于$\infty$时相比上两种有优势，z=0处不可导但是由于0的概率太小所以无所谓

    LeakyReLU(z)=max(0.01z,0)

* 对激活函数求导进行梯度下降法
    令a等于原激活函数在z处的取值

    $\sigma$'(z)=a(1-a)

    tanh(z)=1-a$^2$

    ReLU(z)=0(z<0);1(z>0)

    LeakyReLU(z)=0.01(z<0);1(z>0)
    

### 随机初始化

随机初始化的重要性：防止多个隐藏单位出现对称性

2层神经网络随机初始化的例子：
    注意：随机初始化只针对W[1]

    W[1]=np.random.randn((2,2))*0.01

    b[1].zero((2,2))

    W[2]=...

    b[2].zero((2,2))...

    ps:使用0.01而不是100的原因：
    因为当使用tanh或者sigmoid函数作为隐藏层或者用sigmoid函数作为输出层是时，如果权重太大（z[1]=W[1]x+b[1]，a[1]=g[1](z[1])）
    会落在函数的平缓区/接近饱和（斜率很小）梯度下降法很慢，学习速度变慢
    总之，初始化参数一般都很小就对了。
### 前向和反向传播
* 前向传播
    input a[i-1]
    output a[i],cache z[i]=W[i]x+b[i]

    a[i]=g[i](z[i])

    向量化表示：
    input A[0]
    output A[l],cache z[l]
    z[l]=W[l]A[l-1]+b[l]
    A[l]=g[l](z[l])

* 反向传播
    $$ da=\frac{dL(a,y)}{da}=-yloga-(1-y)log(1-a)
    =\frac{y}{a}+1-\frac{y}{1-a} $$

    $$\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a}*\frac{\partial a}{\partial z} $$

    $$ dz=da*g(z)'\quad dz=a-y$$

    $$ dw=dz*x$$

    $$ db=dz$$
* 矩阵维度

    (w已转置)dw[l],w[l]:(n[l],n[l-1])

    db[l],b[l]:(n[l],1)

    向量化：
    Z[l],A[l],dZ[l],dA[l]:(n[l],m)
### 深度神经网络的概念
## 初步认识深度神经网络
"From simple to complex"图像边缘检测

电路原理：隐藏层越少，需要的神经元越多。因此用深度神经网络会简单很多。
### 参数和超参数
  
参数：W[1],b[1],W[2],b[2]...

超参数：学习率$\alpha$，激活函数，隐藏层数，隐藏单元数

某种程度上，超参数的取值决定了参数的值